<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>Spark API in Depth - Blog</title>
    <link rel="shortcut icon" href="/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:400,700|Noto+Serif:400,400i&display=swap"
        rel="stylesheet">
    
    <link rel="stylesheet"
        href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/styles/arduino-light.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.17.1/build/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <header class="header">
  <div class="blog-title">
    <a href="/" class="logo">Blog</a>
  </div>
  <nav class="navbar">
    <ul class="menu">
      
      
      
      <li class="menu-item">
        
        <a href="/" class="menu-item-link">主页</a>
        
      </li>
      
      
      
      <li class="menu-item">
        
        <a href="/archives" class="menu-item-link">归档</a>
        
      </li>
      
      
      
      <li class="menu-item">
        
        <a href="/me" class="menu-item-link">我</a>
        
      </li>
      
      
      
      <li class="menu-item">
        
        <a href="/atom.xml" class="menu-item-link">订阅</a>
        
      </li>
      
    </ul>
  </nav>
</header>
  <main class="main">
    <article class="post">
    <div class="post-title">
        <h1 class="title">Spark API in Depth</h2>
    </div>
    <div class="post-content">
        <blockquote>
<p>《Spark in Action》Chapter4 </p>
</blockquote>
<p>个人总结： </p>
<ul>
<li>DStream &lt;- a seq of RDDs &lt;- Partitions</li>
<li>key-value RDD = pair RDD  manipulated by <code>funtionByKey()</code></li>
</ul>
<a id="more"></a>
<p><img src="http://ww1.sinaimg.cn/large/005wR1ytgy1fxpu01vvarj30gk07ydg6.jpg" alt=""></p>
<h3 id="pair-RDDs"><a href="#pair-RDDs" class="headerlink" title="pair RDDs"></a>pair RDDs</h3><p>key-value 模型是一种简单、通用的数据模型。</p>
<h4 id="Creating"><a href="#Creating" class="headerlink" title="Creating"></a>Creating</h4><p>利用map将RDD中数据转换成二元组（f(element), element)，即RDD-&gt;pair RDD。</p>
<p>原理：二元组形式的RDD会隐式转换为PairRDDFunctions的实例(Scala语法)。</p>
<h4 id="Basic-pair-RDD-functions"><a href="#Basic-pair-RDD-functions" class="headerlink" title="Basic pair RDD functions"></a>Basic pair RDD functions</h4><h5 id="getting-keys-and-values"><a href="#getting-keys-and-values" class="headerlink" title="getting keys and values"></a>getting keys and values</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pRDD.keys.distinct.count()</span><br></pre></td></tr></table></figure>
<h5 id="counting-values-per-key"><a href="#counting-values-per-key" class="headerlink" title="counting values per key"></a>counting values per key</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pRDD.countByKey()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 求最值</span></span><br><span class="line"><span class="keyword">val</span> (cid, purch) = transByCust.countByKey.toSeq.maxBy(_._2)</span><br></pre></td></tr></table></figure>
<h5 id="looking-up-values-for-a-single-key"><a href="#looking-up-values-for-a-single-key" class="headerlink" title="looking up values for a single key"></a>looking up values for a single key</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transByCust.lookup(<span class="number">53</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 打印结果</span></span><br><span class="line">transByCust.lookup(<span class="number">53</span>).foreach(tran =&gt; println(tran.mkString(<span class="string">", "</span>)))</span><br></pre></td></tr></table></figure>
<h5 id="using-the-mapValues-transformation-to-change-values-in-a-pair-RDD"><a href="#using-the-mapValues-transformation-to-change-values-in-a-pair-RDD" class="headerlink" title="using the mapValues transformation to change values in  a pair RDD"></a>using the mapValues transformation to change values in  a pair RDD</h5><p>只改变值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transByCust = transByCust.mapValues(tran =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (tran(<span class="number">3</span>).toInt == <span class="number">25</span> &amp;&amp; tran(<span class="number">4</span>).toDouble &gt; <span class="number">1</span>)</span><br><span class="line">        tran(<span class="number">5</span>) = (tran(<span class="number">5</span>).toDouble * <span class="number">0.95</span>).toString</span><br><span class="line"></span><br><span class="line">      tran</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>
<h5 id="using-the-flatMapValues-transformation-to-add-values-to-keys"><a href="#using-the-flatMapValues-transformation-to-add-values-to-keys" class="headerlink" title="using the flatMapValues transformation to add values to keys"></a>using the flatMapValues transformation to add values to keys</h5><p>此函数可以给一个键添加多个值或一起移除某个键（键的数量会增减）。</p>
<blockquote>
<p>From each of the values in the return collection, a new key-value pair is created for the corresponding key.</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transByCust = transByCust.flatMapValues(tran =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (tran(<span class="number">3</span>).toInt == <span class="number">81</span> &amp;&amp; tran(<span class="number">4</span>).toDouble &gt;= <span class="number">5</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> cloned = tran.clone()</span><br><span class="line">        cloned(<span class="number">5</span>) = <span class="string">"0.00"</span></span><br><span class="line">        cloned(<span class="number">3</span>) = <span class="string">"70"</span></span><br><span class="line">        cloned(<span class="number">4</span>) = <span class="string">"1"</span></span><br><span class="line">        <span class="type">List</span>(tran, cloned)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">List</span>(tran)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)s</span><br></pre></td></tr></table></figure>
<h5 id="using-reduceByKey-transformation-to-merge-all-values-of-a-key"><a href="#using-reduceByKey-transformation-to-merge-all-values-of-a-key" class="headerlink" title="using reduceByKey transformation to merge all values of a key"></a>using reduceByKey transformation to merge all values of a key</h5><p>foldByKey 与 reduceByKey 类似，区别在于需要一个额外的 zeroValue 参数。</p>
<p><code>foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</code></p>
<p> 由于RDD的并行特性，zeroValues可能被多次使用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> amounts = transByCust.mapValues(t =&gt; t(<span class="number">5</span>).toDouble)</span><br><span class="line"><span class="keyword">val</span> totals = amounts.foldByKey(<span class="number">0</span>)(_ + _).collect()</span><br></pre></td></tr></table></figure>
<h5 id="using-aggregateByKey-to-group-all-values-of-a-key"><a href="#using-aggregateByKey-to-group-all-values-of-a-key" class="headerlink" title="using aggregateByKey to group all values of a key"></a>using aggregateByKey to group all values of a key</h5><h4 id="data-partition-and-reducing-data-shuffling"><a href="#data-partition-and-reducing-data-shuffling" class="headerlink" title="data partition and reducing data shuffling"></a>data partition and reducing data shuffling</h4><p>例如，当从本地文件系统加载文件到Spark时，文件内容被分割成分片，最终被分配到集群的结点上。这些分片形成了RDD，同一结点上可能不止一个分片。每个RDD维护一个分片列表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(transByProd.partitions.length)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The number of RDD partitions is important because, in addition to influencing data distribution throughout the cluster, it also directly determines the number of tasks that will be running RDD transformations. If this number is too small, the cluster will be underutilized. Furthermore, memory problems could result, because working sets might get too big to fit into the memory of executors. We recommend using three to four times more partitions than there are cores in your cluster. Moderately larger values shouldn’t pose a problem, so feel free to experiment. But don’t get too crazy,because management of a large number of tasks could create a bottleneck.</p>
</blockquote>
<h5 id="using-Spark’s-data-partitioners"><a href="#using-Spark’s-data-partitioners" class="headerlink" title="using Spark’s data partitioners"></a>using Spark’s data partitioners</h5><p>两种：HashPartitioner、RangePartitioner。也可定制。</p>
<p>默认是HashPartitioner。</p>
<p>partitionIndex = hashCode % numberOfPartitions</p>
<h5 id="Understanding-and-avoiding-unnecessary-shuffling"><a href="#Understanding-and-avoiding-unnecessary-shuffling" class="headerlink" title="Understanding and avoiding unnecessary shuffling"></a>Understanding and avoiding unnecessary shuffling</h5><blockquote>
<p>Physical movement of data between partitions is called shuffling. </p>
<p>It occurs when data from multiple partitions needs to be combined in order to build partitions <strong>for a new RDD</strong> .When grouping elements by key，shuffling occurs.</p>
</blockquote>
<p>When grouping elements by key，shuffling occurs.</p>
<h5 id="Shuffling-when-explicitly-显式-changing-partitioners"><a href="#Shuffling-when-explicitly-显式-changing-partitioners" class="headerlink" title="Shuffling when explicitly(显式) changing partitioners"></a>Shuffling when explicitly(显式) changing partitioners</h5><blockquote>
<p>Because changing the partitioner provokes shuffles, the safest approach,performance-wise(性能优先), is to use a default partitioner as much as possible and avoidinadvertently causing a shuffle.</p>
</blockquote>
<h5 id="shuffing-caused-by-partitions-removal"><a href="#shuffing-caused-by-partitions-removal" class="headerlink" title="shuffing caused by partitions removal"></a>shuffing caused by partitions removal</h5><p>详细等学Spark优化再看。</p>
<h4 id="Repartitioning-RDDs"><a href="#Repartitioning-RDDs" class="headerlink" title="Repartitioning RDDs"></a>Repartitioning RDDs</h4><h5 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h5><p>只有pair RDD可用，接受一个Partitioner作为参数；当此Partition与原先的不一样时，shuffle发生，重新分片。</p>
<h5 id="collecting-partition-data-with-a-glom-transfotamtion"><a href="#collecting-partition-data-with-a-glom-transfotamtion" class="headerlink" title="collecting partition data with a glom transfotamtion"></a>collecting partition data with a glom transfotamtion</h5><p>glom(意思同 grab)，即将每个分片合成一个数组，用返回的RDD将这些数组作为元素。新RDD的数组数量等于之前的分片数量。这个过程中，partitioner被移除。</p>
<h4 id="Joining-sorting-and-grouping-data"><a href="#Joining-sorting-and-grouping-data" class="headerlink" title="Joining, sorting, and grouping data"></a>Joining, sorting, and grouping data</h4><p>join((K, (V, W))) 非空</p>
<p>leftOuterJoin  (K, (V, Option(W)))</p>
<p>rightOuterJoin (K, (Option(V), W))</p>
<p>fullOuterJoin (K, (Option(V), Option(W))</p>
<h5 id="using-subtract-差集"><a href="#using-subtract-差集" class="headerlink" title="using subtract(差集)"></a>using subtract(差集)</h5><p>…</p>
<h4 id="Using-accumulators-and-broadcast-variables-to-communicate-with-Spark-executors"><a href="#Using-accumulators-and-broadcast-variables-to-communicate-with-Spark-executors" class="headerlink" title="Using accumulators and broadcast variables to communicate with Spark executors"></a>Using accumulators and broadcast variables to communicate with Spark executors</h4><p>作用：维护全局状态或在任务和分片之间共享数据。</p>
<h5 id="sending-data-to-executors-using-broadcast-variables-bv"><a href="#sending-data-to-executors-using-broadcast-variables-bv" class="headerlink" title="sending data to executors using broadcast variables(bv)"></a>sending data to executors using broadcast variables(bv)</h5><p>能在集群间共享，与accumulators不同的是，其不能被执行器修改。驱动创造bv，然后执行器读取它。</p>
<p>当有一个大集合数据需要被绝大多数执行器使用时，应使用bv。</p>
<p>创建：<code>SparkContext.broadcast(value)</code>，读<code>Broadcast.value</code>。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li><p>pair RDD含二元组：keys 和 values。</p>
</li>
<li><p>scala中pair RDD隐式转换为pairRDDFuctions的实例，它有专有的pair RDD操作。</p>
</li>
<li><p>countByKey返回map，含每个键出现的次数。</p>
</li>
<li><p>mapValues，只改变pair RDD的值。</p>
</li>
<li><p>flatMapValues，一个键能对应零个或多个值（键的总数增加）。</p>
</li>
<li><p>reduceByKey和foldByKey，归约同一个键的所有值到一个值，值类型不变。</p>
</li>
<li><p>aggregateByKey，聚合值，但转换值到其它类型。</p>
</li>
<li><p>Data Partition，是Spark的一种在一个集群的多个结点间分数据的机制。</p>
</li>
<li><blockquote>
<p>The number of RDD partitions is important because, in addition to influencing<br>data distribution throughout the cluster, it also directly determines the number<br>of tasks that will be running RDD transformations.</p>
</blockquote>
</li>
<li><p>shffuling时，数据不仅被写到硬盘，而且在网络之间传输。因此，使Spark作业中的shuffle数最小化是重要的。</p>
</li>
<li><blockquote>
<p>Every Spark job is divided into stages based on the points where shuffles occur.</p>
</blockquote>
</li>
</ul>

    </div>
    <div class="post-meta">
        <span class="post-time">2018-11-30</span>
    </div>
</article>

<div class="prev_next">
    
<div class="prev">
    
    <a href="/2018/12/17/bigdata/SparkStreaming/" class="alignleft prev"
        title="Spark Streaming base on Kafka">&lt;= Spark Streaming base on Kafka</a>
    
</div>
<div class="next">
    
    <a href="/2018/11/20/python/problem/" class="alignright next"
        title="Python编程中邂逅的问题">Python编程中邂逅的问题 =&gt;</a>
    
</div>

</div>

<div class="post-comment">
    


</div>
  </main>
</body>

</html>